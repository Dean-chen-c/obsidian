
## 前言

企业云部署时 Global KS 服务默认都是在同一个机房内存在主备高可用的（cronus api influxdb 除外，产研推进中），可以实现在机房出现仅个别物理机出故障时单节点服务。

但在遇到机房整体故障时 Global KS 服务就无法工作，导致还有正常运转的可用区没办法做业务管理，遂推出 Global KS 高可用主备双机房的架构来保证云平台的业务管理工作。

### 何时需要 Global KS 高可用方案？

1. 客户一个云平台中至少有两个 SubZone；
2. 该云平台有较高的全局管理操作频率；

## Global KS 高可用标准部署架构

目前我们仅支持两种：

- （主推架构）跨 Region 两机房 Global 独立部署，简称 “非 Region 独立”
- Region 内两机房 Global+Sub 融合部署，简称 “Region 融合”

### （主推架构）非 Region 独立部署

分别为 Global 服务单独部署两个单 Zone 的 Region 环境，每个 Zone 的 is_global_zone == 1 且 is_sub_zone == 0。

只有一个 Zone 提供活跃的 Global KS 服务（主机房），另一个 Zone Standby（备机房） ，可按需手动切换 Global KS 到备机房运行。

两个 Zone 的 global pg 节点组成一个集群，主机房 Zone 内数据采用同步流复制，Zone 间及备机房采用异步流复制主机房的 pg 数据。

![gh](https://cdn.jsdelivr.net/gh/Dean-chen-c/obsidian@main/main/images/1688607076000zrtzam.png)


#### **优点：**

1. 无需考虑机房间的网络延迟，支持同城或异地部署；
2. 机房仅运行 Global KS 服务，通常不会与业务故障同频；
3. Global KS 服务本身较低频不会有太多性能问题，即使出现资源问题扩容方案也会较为简单；

#### **缺点：**

1. 每个 Global Zone 至少需要 3 台物理节点，如果原本是 Global+Sub 融合的环境，改造后需增加至少 6 台物理机；
2. 当原本是 Global+Sub 融合的环境时，需要先拆除原环境的 Global 然后新建两个 Global Region ，改造服务成本较高；

#### **目前存在的问题：**

1. 仅在企业云版本 V5.1（20211213）上有过验证及试点客户实施，尚未形成标准产品方案（研发推进中，产研先继续支持 5.1 版本，后续再考虑版本升级适配）；
2. global ks 高可用运维切换命令暂不支持一键切换，需要参照[[g]]中逐条执行（研发推进中）；
3. 方案中尚未包含 boss 高可用和 cronus-api influxdb 的高可用（研发推进中）；
4. 当前方案上线后会卸载原有的 pg_watcher ，因此对原本 Zone 内的 pg 高可用将造成影响（研发推进中）。

#### **适用场景：**

1. **有 Global KS 双机房高可用需求规划的全新客户；**
2. **已有多 Region 部署架构的客户；**
3. **需要对 Global KS 服务进行异地容灾的客户；**
4. **业务节点有一定规模（> 15 hyper）的客户；**
5. **对业务操作敏感度和业务运行敏感度均有较高要求的客户。**

### （勉强架构）Region 融合部署

Region 内有两个 Zone ，每个 Zone 的 is_global_zone == 1 且 is_sub_zone == 1。

只有一个 Zone 提供活跃的 Global KS 服务（主机房），另一个 Zone Standby（备机房） ，可按需手动切换 Global KS 到备机房运行。

两个 Zone 的 global pg 节点组成一个集群，数据采用同步流复制。

![gh](https://cdn.jsdelivr.net/gh/Dean-chen-c/obsidian@main/main/images/168860709900006xxgo.png)

#### **优点：**

1. 对于已有 Region 两机房的客户来说，只需要在非 Global 区改造复用，不用增加物理节点，节约成本；
2. pg 集群采用同步流复制，会有相对更完整的数据一致性；

#### **缺点：**

1. 部署时对两机房的网络延迟有要求（一般同城裸纤，小于 5 ms），无法达到网络要求的环境不能选择此架构；
2. Global+Sub 融合在遇到机房故障时同时会存在同机房业务故障，这种部署模式只能做到主机房故障后备机房的业务数据依然能被管理；
3. 当出现城域网故障，则该 Region 下 Global 和 Sub 都不可用，如果还有其他正常运转的 SubZone 依然无法得到管理；
4. 由于 Global+Sub 融合将资源利用到极致，因此更容易出现因资源（例如 CPU / 内存 / 磁盘容量或 IO 不足等）导致的业务性能问题，在有需要时的扩容方案也会特别复杂（需要考虑的风险和因素比较多）；
5. 未来需要演进为“非 Region 独立”架构时（例如规模扩大），得先拆两个机房的 Global 服务然后新建两个 Global Region ，改造服务成本极高；

#### **目前存在的问题：**

1. 仅在企业云版本 V5.1（20211213）上有过验证及试点客户实施，暂未形成标准产品方案（研发推进中，产研先继续支持 5.1 版本，后续再考虑版本升级适配）；
2. global ks 高可用运维切换命令暂不支持一键切换，需要参照《[[Global KS 高可用 - region 两机房融合部署运维]]》中逐条执行（研发推进中）；
3. 备机房出现故障时，也需要做 pg 数据库的回切操作以剔除备机房不可用的 pg 节点，以避免造成 pg 集群的同步流复制异常导致操作只读；
4. 方案中尚未包含 boss 高可用和 cronus-api influxdb 的高可用（研发推进中）；
5. 当前方案上线后会卸载原有的 pg_watcher ，因此对原本 Zone 内的 pg 高可用将造成影响（研发推进中）。

#### 适用场景：

**仅适用**于已有同城两机房 Region 环境、且已存在 Global+Sub 融合部署、且业务节点规模不大（＜ 15 Hyper）、且对业务操作敏感度高于业务运行敏感度的客户。

## 附：企业云支持的部署架构及演进路线

![gh](https://cdn.jsdelivr.net/gh/Dean-chen-c/obsidian@main/main/images/1688607114000muubex.png)
